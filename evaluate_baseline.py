#!/usr/bin/env python3
"""
Qwen2-VL íŒŒì¸íŠœë‹ ì „ ëª¨ë¸ í‰ê°€
ì—¬ëŸ¬ ìƒ˜í”Œë¡œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
"""

import torch
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from PIL import Image
import json
from tqdm import tqdm

print("ğŸ“¦ ëª¨ë¸ ë¡œë”© ì¤‘...")
model = Qwen2VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2-VL-2B-Instruct",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True,
)

processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-2B-Instruct")
print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ\n")

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
print("ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”© ì¤‘...")
with open("/root/data/deepseek_ocr/val_qwen2vl.jsonl", 'r', encoding='utf-8') as f:
    test_data = [json.loads(line) for line in f]

# 20ê°œ ìƒ˜í”Œë§Œ í…ŒìŠ¤íŠ¸
test_samples = test_data[:20]
print(f"í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {len(test_samples)}ê°œ\n")

# í‰ê°€ ì‹œì‘
print("ğŸ” ëª¨ë¸ í‰ê°€ ì‹œì‘...")
print("="*80)

correct = 0
total = 0
results = []

for idx, item in enumerate(tqdm(test_samples, desc="í‰ê°€ ì¤‘")):
    messages = item["messages"]
    
    # ì •ë‹µ ì¶”ì¶œ
    image_path = messages[0]["content"][0]["image"]
    user_text = messages[0]["content"][1]["text"]
    ground_truth = messages[1]["content"][0]["text"]
    
    # ì´ë¯¸ì§€ ë¡œë“œ
    try:
        image = Image.open(image_path).convert("RGB")
    except Exception as e:
        print(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {image_path}")
        continue
    
    # ëŒ€í™” êµ¬ì„±
    conversation = [
        {
            "role": "user",
            "content": [
                {"type": "image"},
                {"type": "text", "text": user_text}
            ]
        }
    ]
    
    # ì¶”ë¡ 
    text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)
    inputs = processor(
        text=[text_prompt],
        images=[image],
        padding=True,
        return_tensors="pt"
    ).to("cuda")
    
    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=128,
            do_sample=False,
        )
    
    generated_text = processor.batch_decode(output_ids, skip_special_tokens=True)[0]
    
    # ì‘ë‹µ ì¶”ì¶œ
    if "assistant\n" in generated_text:
        prediction = generated_text.split("assistant\n")[-1].strip()
    else:
        prediction = generated_text.split("assistant")[-1].strip() if "assistant" in generated_text else generated_text
    
    # ì •í™•ë„ ê³„ì‚° (ì™„ì „ ì¼ì¹˜)
    is_correct = prediction.strip() == ground_truth.strip()
    if is_correct:
        correct += 1
    total += 1
    
    results.append({
        "index": idx + 1,
        "ground_truth": ground_truth,
        "prediction": prediction,
        "correct": is_correct
    })

# ê²°ê³¼ ì¶œë ¥
print("\n" + "="*80)
print("ğŸ“Š í‰ê°€ ê²°ê³¼")
print("="*80)
print(f"ì´ ìƒ˜í”Œ: {total}ê°œ")
print(f"ì •í™•íˆ ë§ì¶˜ ìƒ˜í”Œ: {correct}ê°œ")
print(f"ì •í™•ë„: {correct/total*100:.2f}%")
print("="*80)

# ìƒ˜í”Œ ê²°ê³¼ ì¶œë ¥
print("\nğŸ“ ìƒ˜í”Œ ê²°ê³¼ (ì²˜ìŒ 10ê°œ):")
print("-"*80)
for result in results[:10]:
    status = "âœ…" if result["correct"] else "âŒ"
    print(f"\n{status} ìƒ˜í”Œ {result['index']}:")
    print(f"   ì •ë‹µ: {result['ground_truth']}")
    print(f"   ì˜ˆì¸¡: {result['prediction']}")

# ê²°ê³¼ ì €ì¥
output_file = "/root/data/qwen2vl_baseline_results.json"
with open(output_file, 'w', encoding='utf-8') as f:
    json.dump({
        "total": total,
        "correct": correct,
        "accuracy": correct/total*100,
        "results": results
    }, f, ensure_ascii=False, indent=2)

print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")
print("\nâœ… í‰ê°€ ì™„ë£Œ!")
