#!/usr/bin/env python3
"""
Qwen2-VL íŒŒì¸íŠœë‹ with ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
"""

import torch
import gc
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from transformers import BitsAndBytesConfig
from PIL import Image
import json
from tqdm import tqdm
import os

def get_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        return f"í• ë‹¹: {allocated:.2f}GB, ì˜ˆì•½: {reserved:.2f}GB"
    return "N/A"

def clear_memory():
    """ë©”ëª¨ë¦¬ ì •ë¦¬"""
    gc.collect()
    torch.cuda.empty_cache()

print("="*70)
print("ğŸ¯ Qwen2-VL íŒŒì¸íŠœë‹ with ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§")
print("="*70)

# ì´ˆê¸° ë©”ëª¨ë¦¬
print(f"ğŸ” ì´ˆê¸° GPU ë©”ëª¨ë¦¬: {get_gpu_memory()}")

# ë©”ëª¨ë¦¬ ì •ë¦¬
clear_memory()
print(f"ğŸ§¹ ì •ë¦¬ í›„: {get_gpu_memory()}")

# 4ë¹„íŠ¸ ì–‘ìí™”
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

print("\nğŸ“¦ ëª¨ë¸ ë¡œë”© ì¤‘...")
model = Qwen2VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2-VL-2B-Instruct",
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
    local_files_only=True,  # ìºì‹œë§Œ ì‚¬ìš©
)
print(f"   GPU ë©”ëª¨ë¦¬: {get_gpu_memory()}")

processor = AutoProcessor.from_pretrained(
    "Qwen/Qwen2-VL-2B-Instruct",
    local_files_only=True,  # ìºì‹œë§Œ ì‚¬ìš©
)

# LoRA ì ìš©
print("\nğŸ”§ LoRA ì ìš© ì¤‘...")
model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=False)

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
print(f"   GPU ë©”ëª¨ë¦¬: {get_gpu_memory()}")

# ë°ì´í„° ë¡œë“œ (5ê°œë§Œ)
print("\nğŸ“Š ë°ì´í„° ë¡œë”© ì¤‘...")
with open("/root/data/deepseek_ocr/train_qwen2vl.jsonl", 'r', encoding='utf-8') as f:
    train_data = [json.loads(line) for line in f]

train_samples = train_data[:5]  # 5ê°œë§Œ
print(f"í•™ìŠµ ìƒ˜í”Œ: {len(train_samples)}ê°œ\n")

# ì˜µí‹°ë§ˆì´ì €
optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=2e-4)

# í•™ìŠµ
print("ğŸš€ í•™ìŠµ ì‹œì‘!")
print("-"*70)

model.train()
total_loss = 0
success_count = 0

for idx, item in enumerate(train_samples):
    try:
        print(f"\n[ìƒ˜í”Œ {idx+1}/{len(train_samples)}]")
        print(f"  í•™ìŠµ ì „ ë©”ëª¨ë¦¬: {get_gpu_memory()}")
        
        messages = item["messages"]
        
        # ë°ì´í„° ì¶”ì¶œ
        image_path = messages[0]["content"][0]["image"]
        user_text = messages[0]["content"][1]["text"]
        assistant_text = messages[1]["content"][0]["text"]
        
        print(f"  ì •ë‹µ: {assistant_text}")
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        image = Image.open(image_path).convert("RGB")
        
        # ëŒ€í™” êµ¬ì„±
        conversation = [
            {
                "role": "user",
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": user_text}
                ]
            },
            {
                "role": "assistant",
                "content": [
                    {"type": "text", "text": assistant_text}
                ]
            }
        ]
        
        # í”„ë¡œì„¸ì‹±
        text_prompt = processor.apply_chat_template(conversation, tokenize=False)
        inputs = processor(
            text=[text_prompt],
            images=[image],
            padding=True,
            return_tensors="pt"
        ).to(model.device)
        
        print(f"  ì…ë ¥ ì²˜ë¦¬ í›„: {get_gpu_memory()}")
        
        inputs["labels"] = inputs["input_ids"].clone()
        
        # Forward
        outputs = model(**inputs)
        loss = outputs.loss
        
        print(f"  Forward í›„: {get_gpu_memory()}")
        print(f"  Loss: {loss.item():.4f}")
        
        # Backward
        loss.backward()
        
        print(f"  Backward í›„: {get_gpu_memory()}")
        
        # Update
        optimizer.step()
        optimizer.zero_grad()
        
        total_loss += loss.item()
        success_count += 1
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del inputs, outputs, loss, image
        clear_memory()
        
        print(f"  ì •ë¦¬ í›„ ë©”ëª¨ë¦¬: {get_gpu_memory()}")
        print(f"  âœ… ì„±ê³µ")
        
    except torch.cuda.OutOfMemoryError as e:
        print(f"  âŒ OOM ì—ëŸ¬!")
        print(f"  ë©”ëª¨ë¦¬: {get_gpu_memory()}")
        clear_memory()
        print(f"  ì •ë¦¬ í›„: {get_gpu_memory()}")
        continue
    except Exception as e:
        print(f"  âŒ ì—ëŸ¬: {e}")
        continue

print("\n" + "="*70)
if success_count > 0:
    avg_loss = total_loss / success_count
    print(f"âœ… í•™ìŠµ ì™„ë£Œ!")
    print(f"   - ì„±ê³µ: {success_count}/{len(train_samples)}")
    print(f"   - í‰ê·  Loss: {avg_loss:.4f}")
    print(f"   - ìµœì¢… ë©”ëª¨ë¦¬: {get_gpu_memory()}")
    
    # ëª¨ë¸ ì €ì¥
    print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
    save_dir = "/root/data/qwen2vl_tiny_finetuned"
    os.makedirs(save_dir, exist_ok=True)
    
    model.save_pretrained(save_dir)
    processor.save_pretrained(save_dir)
    
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {save_dir}")
else:
    print(f"âŒ ëª¨ë“  ìƒ˜í”Œ ì‹¤íŒ¨")

print("="*70)
