#!/usr/bin/env python3
"""
Qwen2-VL íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸ (Unsloth ì‚¬ìš©)
í•œêµ­ì–´ ê°„íŒ OCR í”„ë¡œì íŠ¸
"""

import torch
from unsloth import FastVisionModel
from datasets import load_dataset
from transformers import TextStreamer
from unsloth import is_bf16_supported
from unsloth.trainer import UnslothVisionDataCollator
from trl import SFTTrainer, SFTConfig

# 4ë¹„íŠ¸ ì–‘ìí™”ë¡œ ëª¨ë¸ ë¡œë“œ
model, tokenizer = FastVisionModel.from_pretrained(
    "unsloth/Qwen2-VL-2B-Instruct",
    load_in_4bit=True,
    use_gradient_checkpointing=False,  # gradient checkpointing ë¹„í™œì„±í™”
)

# LoRA ì–´ëŒ‘í„° ì¶”ê°€ (language layersë§Œ íŒŒì¸íŠœë‹)
model = FastVisionModel.get_peft_model(
    model,
    finetune_vision_layers=False,  # visionì€ ê³ ì •
    finetune_language_layers=True,  # languageë§Œ í•™ìŠµ
    finetune_attention_modules=True,
    finetune_mlp_modules=True,
    
    r=16,
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    random_state=42,
    use_rslora=False,
    loftq_config=None,
)

# ë°ì´í„°ì…‹ ë¡œë“œ
train_dataset = load_dataset("json", data_files="/root/data/deepseek_ocr/train_qwen2vl.jsonl", split="train")
val_dataset = load_dataset("json", data_files="/root/data/deepseek_ocr/val_qwen2vl.jsonl", split="train")

print(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ")
print(f"- í•™ìŠµ ìƒ˜í”Œ: {len(train_dataset)}ê°œ")
print(f"- ê²€ì¦ ìƒ˜í”Œ: {len(val_dataset)}ê°œ")

# ë°ì´í„° ì½œë ˆì´í„° ì„¤ì •
data_collator = UnslothVisionDataCollator(model, tokenizer)

# SFT íŠ¸ë ˆì´ë„ˆ ì„¤ì •
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    data_collator=data_collator,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    args=SFTConfig(
        per_device_train_batch_size=2,
        per_device_eval_batch_size=2,
        gradient_accumulation_steps=4,
        
        warmup_steps=50,
        num_train_epochs=1,
        max_steps=100,  # ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 100 ìŠ¤í…ë§Œ
        
        learning_rate=2e-4,
        fp16=not is_bf16_supported(),
        bf16=is_bf16_supported(),
        
        logging_steps=10,
        eval_steps=25,
        save_steps=25,
        save_total_limit=3,
        
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="linear",
        seed=42,
        
        output_dir="/root/data/qwen2vl_korean_signboard",
        report_to="none",
        
        remove_unused_columns=False,
        dataset_text_field="",
        dataset_kwargs={"skip_prepare_dataset": True},
        dataset_num_proc=4,
        max_seq_length=2048,
    ),
)

# í•™ìŠµ ì‹œì‘
print("\nğŸš€ í•™ìŠµ ì‹œì‘!")
trainer_stats = trainer.train()

# ëª¨ë¸ ì €ì¥
print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
model.save_pretrained("/root/data/qwen2vl_korean_signboard/final")
tokenizer.save_pretrained("/root/data/qwen2vl_korean_signboard/final")

print("\nâœ… í•™ìŠµ ì™„ë£Œ!")
print(f"- í•™ìŠµ Loss: {trainer_stats.training_loss:.4f}")
print(f"- ì´ ìŠ¤í…: {trainer_stats.global_step}")
print(f"- ì €ì¥ ìœ„ì¹˜: /root/data/qwen2vl_korean_signboard/final")

# ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸
print("\nğŸ§ª ì¶”ë¡  í…ŒìŠ¤íŠ¸...")
FastVisionModel.for_inference(model)

# ì²« ë²ˆì§¸ ê²€ì¦ ìƒ˜í”Œë¡œ í…ŒìŠ¤íŠ¸
test_sample = val_dataset[0]
test_messages = test_sample["messages"]
test_image = test_messages[0]["content"][0]["image"]
test_question = test_messages[0]["content"][1]["text"]
correct_answer = test_messages[1]["content"][0]["text"]

inputs = tokenizer.apply_chat_template(
    [{"role": "user", "content": [{"type": "image", "image": test_image}, {"type": "text", "text": test_question}]}],
    add_generation_prompt=True,
    tokenize=True,
    return_tensors="pt",
    return_dict=True,
).to("cuda")

text_streamer = TextStreamer(tokenizer, skip_prompt=True)
generated = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128, 
                          use_cache=True, temperature=0.5, min_p=0.1)

print(f"\nì •ë‹µ: {correct_answer}")
