#!/usr/bin/env python3
"""
DeepSeek-OCR íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸
ê³µì‹ í•™ìŠµ ì½”ë“œ ì—†ì´ ì§ì ‘ êµ¬í˜„
"""

import torch
import torch.nn as nn
from transformers import AutoTokenizer
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from transformers import BitsAndBytesConfig
from PIL import Image
import json
from tqdm import tqdm
import os
import sys

# DeepSeek-OCR ëª¨ë¸ ì„í¬íŠ¸
sys.path.insert(0, '/root/data/deepseek_ocr_model')
from modeling_deepseekocr import DeepseekOCRForCausalLM, DeepseekOCRConfig

def get_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        return f"í• ë‹¹: {allocated:.2f}GB, ì˜ˆì•½: {reserved:.2f}GB"
    return "N/A"

print("="*70)
print("ğŸ¯ DeepSeek-OCR íŒŒì¸íŠœë‹")
print("="*70)

print(f"\nğŸ” ì´ˆê¸° GPU ë©”ëª¨ë¦¬: {get_gpu_memory()}")

# 4ë¹„íŠ¸ ì–‘ìí™” ì„¤ì •
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# ëª¨ë¸ ë¡œë“œ
print("\nğŸ“¦ DeepSeek-OCR ëª¨ë¸ ë¡œë”© ì¤‘...")
try:
    model = DeepseekOCRForCausalLM.from_pretrained(
        "/root/data/deepseek_ocr_model",
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        local_files_only=True,
    )
    print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    print(f"   GPU ë©”ëª¨ë¦¬: {get_gpu_memory()}")
except Exception as e:
    print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    print("\n4ë¹„íŠ¸ ì—†ì´ ì¬ì‹œë„...")
    model = DeepseekOCRForCausalLM.from_pretrained(
        "/root/data/deepseek_ocr_model",
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
        local_files_only=True,
    )
    print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ (float16)")
    print(f"   GPU ë©”ëª¨ë¦¬: {get_gpu_memory()}")

# Tokenizer ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(
    "/root/data/deepseek_ocr_model",
    trust_remote_code=True,
    local_files_only=True,
)

# LoRA ì ìš©
print("\nğŸ”§ LoRA ì ìš© ì¤‘...")
try:
    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=False)
    
    lora_config = LoraConfig(
        r=8,
        lora_alpha=16,
        target_modules=["q_proj", "v_proj", "o_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
    )
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    print(f"   GPU ë©”ëª¨ë¦¬: {get_gpu_memory()}")
except Exception as e:
    print(f"âš ï¸ LoRA ì ìš© ì‹¤íŒ¨: {e}")
    print("LoRA ì—†ì´ ì§„í–‰...")

# ë°ì´í„° ë¡œë“œ (ë§¤ìš° ì‘ì€ ìƒ˜í”Œ)
print("\nğŸ“Š ë°ì´í„° ë¡œë”© ì¤‘...")
with open("/root/data/deepseek_ocr/train.jsonl", 'r', encoding='utf-8') as f:
    train_data = [json.loads(line) for line in f]

train_samples = train_data[:3]  # 3ê°œë§Œ
print(f"í•™ìŠµ ìƒ˜í”Œ: {len(train_samples)}ê°œ\n")

# ì˜µí‹°ë§ˆì´ì €
optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=2e-4)

# í•™ìŠµ
print("ğŸš€ í•™ìŠµ ì‹œì‘!")
print("-"*70)

model.train()
total_loss = 0
success_count = 0

for idx, item in enumerate(train_samples):
    try:
        print(f"\n[ìƒ˜í”Œ {idx+1}/{len(train_samples)}]")
        print(f"  í•™ìŠµ ì „ ë©”ëª¨ë¦¬: {get_gpu_memory()}")
        
        # ë°ì´í„° ì¶”ì¶œ
        image_path = item['image']
        conversations = item['conversations']
        
        # ëŒ€í™”ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        user_text = conversations[0]['content']
        assistant_text = conversations[1]['content']
        
        print(f"  ì •ë‹µ: {assistant_text}")
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        image = Image.open(f"/root/data/deepseek_ocr/{image_path}").convert("RGB")
        
        # DeepSeek-OCR ì…ë ¥ í˜•ì‹
        prompt = f"USER: <image>\n{user_text}\nASSISTANT: {assistant_text}"
        
        # ì „ì²˜ë¦¬ (DeepSeek-OCRì˜ process ë©”ì„œë“œ ì‚¬ìš©)
        inputs = model.build_conversation_input_ids(
            tokenizer,
            query=user_text,
            images=[image],
            history=[],
        )
        
        # GPUë¡œ ì´ë™
        input_ids = inputs['input_ids'].to(model.device)
        pixel_values = inputs['pixel_values'].to(model.device)
        images_seq_mask = inputs['images_seq_mask'].to(model.device)
        images_spatial_crop = inputs['images_spatial_crop']
        
        print(f"  ì…ë ¥ ì²˜ë¦¬ í›„: {get_gpu_memory()}")
        
        # labels ì„¤ì •
        labels = input_ids.clone()
        
        # Forward
        outputs = model(
            input_ids=input_ids,
            pixel_values=pixel_values,
            images_seq_mask=images_seq_mask,
            images_spatial_crop=images_spatial_crop,
            labels=labels,
        )
        
        loss = outputs.loss
        
        print(f"  Forward í›„: {get_gpu_memory()}")
        print(f"  Loss: {loss.item():.4f}")
        
        # Backward
        loss.backward()
        
        print(f"  Backward í›„: {get_gpu_memory()}")
        
        # Update
        optimizer.step()
        optimizer.zero_grad()
        
        total_loss += loss.item()
        success_count += 1
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del inputs, input_ids, pixel_values, images_seq_mask, outputs, loss
        torch.cuda.empty_cache()
        
        print(f"  ì •ë¦¬ í›„ ë©”ëª¨ë¦¬: {get_gpu_memory()}")
        print(f"  âœ… ì„±ê³µ")
        
    except torch.cuda.OutOfMemoryError as e:
        print(f"  âŒ OOM ì—ëŸ¬!")
        print(f"  ë©”ëª¨ë¦¬: {get_gpu_memory()}")
        torch.cuda.empty_cache()
        continue
    except Exception as e:
        print(f"  âŒ ì—ëŸ¬: {e}")
        import traceback
        traceback.print_exc()
        continue

print("\n" + "="*70)
if success_count > 0:
    avg_loss = total_loss / success_count
    print(f"âœ… í•™ìŠµ ì™„ë£Œ!")
    print(f"   - ì„±ê³µ: {success_count}/{len(train_samples)}")
    print(f"   - í‰ê·  Loss: {avg_loss:.4f}")
    print(f"   - ìµœì¢… ë©”ëª¨ë¦¬: {get_gpu_memory()}")
    
    # ëª¨ë¸ ì €ì¥
    print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
    save_dir = "/root/data/deepseek_ocr_finetuned"
    os.makedirs(save_dir, exist_ok=True)
    
    try:
        model.save_pretrained(save_dir)
        tokenizer.save_pretrained(save_dir)
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {save_dir}")
    except Exception as e:
        print(f"âš ï¸ ì €ì¥ ì‹¤íŒ¨: {e}")
else:
    print(f"âŒ ëª¨ë“  ìƒ˜í”Œ ì‹¤íŒ¨")

print("="*70)
