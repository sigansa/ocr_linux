# Qwen2-VL ν•κµ­μ–΄ κ°„ν OCR νμΈνλ‹ ν”„λ΅μ νΈ

## π“ ν„μ¬ μƒν™© μ”μ•½

### β… μ™„λ£λ μ‘μ—…
1. **λ°μ΄ν„° μ¤€λΉ„**
   - ν•™μµ λ°μ΄ν„°: 138,149κ° μƒν”
   - κ²€μ¦ λ°μ΄ν„°: 17,272κ° μƒν”
   - Qwen2-VL ν•μ‹μΌλ΅ λ³€ν™ μ™„λ£

2. **λ² μ΄μ¤λΌμΈ ν‰κ°€** (νμΈνλ‹ μ „)
   - β μ™„μ „ μΌμΉ: **10.00%** (20κ° μ¤‘ 2κ°)
   - β… μ •κ·ν™” μΌμΉ: **35.00%** (20κ° μ¤‘ 7κ°) - λ€μ†λ¬Έμ/κ³µλ°± λ¬΄μ‹
   - β… κ³ μ μ‚¬λ„ 80%+: **40.00%** (20κ° μ¤‘ 8κ°) - μ‹¤μ©μ  μ •ν™•λ„
   
   μ£Όμ” λ¬Έμ :
     - λ€μ†λ¬Έμ λ¶μΌμΉ (kokoro β†’ KOKORO) - μ •κ·ν™”λ΅ ν•΄κ²° κ°€λ¥
     - λ„μ–΄μ“°κΈ° μ°¨μ΄ (λ§μ™•μ΅±λ° β†’ λ§μ™• μ΅±λ°) - μ •κ·ν™”λ΅ ν•΄κ²° κ°€λ¥
     - λ¶ν•„μ”ν• μ •λ³΄ ν¬ν•¨ (μ „ν™”λ²νΈ, μμ–΄ λ“±) - νμΈνλ‹ ν•„μ”
     - μ™„μ „ν μλ» μ½μ (λν†µλ§λλ³΄μ β†’ DBAS) - νμΈνλ‹ ν•„μ”

### β λ΅μ»¬ νμΈνλ‹ μ‹¤ν¨ μ›μΈ (μƒμ„Έ)

#### 1. Unsloth λ°©μ‹ μ‹¤ν¨
**μ‹λ„:** `FastVisionModel.from_pretrained()` + LoRA
```python
# train_qwen2vl.py μ‹¤ν–‰ μ‹
```

**μ—λ¬:**
- **Triton μ»΄νμΌ μ¤λ¥**: `triton.backends.nvidia.driver.py` μ΄κΈ°ν™” μ‹¤ν¨
- **μ›μΈ**: Unslothμ μ»¤μ¤ν…€ μ»¤λ„ μµμ ν™”κ°€ Qwen2-VLμ vision attentionκ³Ό μ¶©λ
- **λ°μƒ μ„μΉ**: `apply_rotary_pos_emb_vision()` ν•¨μμ—μ„ torch.compile μ‹λ„ μ¤‘
- **κ·Όλ³Έ μ›μΈ**: Vision-Language λ¨λΈμ λ³µμ΅ν• attention λ©”μ»¤λ‹μ¦ + Unsloth μµμ ν™” λ¶μΌμΉ

**μ¬ν„ κ°€λ¥:**
```bash
cd /root/data && python train_qwen2vl.py
# μ—λ¬: InductorError in apply_rotary_pos_emb_vision
```

---

#### 2. κΈ°λ³Έ Transformers λ°©μ‹ μ‹¤ν¨
**μ‹λ„:** `Transformers` + `PEFT` + `BitsAndBytesConfig`

**λ¬Έμ μ :**

**A. λ°μ΄ν„° μ²λ¦¬ λ³µμ΅μ„±**
- **μ—λ¬**: `ValueError: Mismatch in image token count`
- **μ›μΈ**: Qwen2-VLμ μ΄λ―Έμ§€ ν† ν°ν™” λ©”μ»¤λ‹μ¦μ΄ λ³µμ΅
  - μ΄λ―Έμ§€λ¥Ό κ°€λ³€ ν¬κΈ° ν† ν°μΌλ΅ λ³€ν™
  - `<|vision_start|>`, `<|image_pad|>`, `<|vision_end|>` νΉμ ν† ν°
  - truncationκ³Ό paddingμ΄ μ΄λ―Έμ§€ ν† ν° μμ™€ λ¶μΌμΉ
  
**B. λ©”λ¨λ¦¬ λ¬Έμ **
```
μ‹¤μΈ΅ λ©”λ¨λ¦¬ μ‚¬μ©λ‰ (nvidia-smi):
- μ΄κΈ°: 0GB
- λ¨λΈ λ΅λ”© (4bit): 1.51GB
- LoRA μ μ© ν›„: 2.21GB (μμ•½)
- μ²« μƒν” ν•™μµ μ‹λ„: 23.7GB/24GB (96.5%)
- κ²°κ³Ό: CUDA Out of Memory
```

**μ™ λ©”λ¨λ¦¬κ°€ ν­μ¦ν•λ”κ°€?**
1. **Vision Encoderμ λ©”λ¨λ¦¬ μ”κµ¬λ‰**
   - ViT κΈ°λ° μΈμ½”λ”: μ΄λ―Έμ§€ β†’ νΉμ§• λ²΅ν„°
   - Gradient κ³„μ‚° μ‹ μ¤‘κ°„ activation μ €μ¥
   - μ΄λ―Έμ§€ ν¬κΈ°μ— λΉ„λ΅€ν•΄μ„ λ©”λ¨λ¦¬ μ¦κ°€

2. **λ©€ν‹°λ¨λ‹¬ μµν•©**
   - Vision νΉμ§• + Language μ„λ² λ”© κ²°ν•©
   - Cross-attention λ μ΄μ–΄μ activation
   - λ‘ λ¨λ‹¬λ¦¬ν‹°μ gradient λ™μ‹ λ³΄κ΄€

3. **4λΉ„νΈ μ–‘μν™”μ ν•κ³„**
   - λ¨λΈ κ°€μ¤‘μΉλ” μ‘μ•„μ§ (2.2B β†’ 1.5GB)
   - **ν•μ§€λ§ activationκ³Ό gradientλ” full precision** (bfloat16)
   - ν•™μµ μ‹ activation λ©”λ¨λ¦¬κ°€ μ£Όλ²”

**C. μ†λ„ λ¬Έμ **
```
μ‹¤μΈ΅ μ†λ„ (train_with_memory_monitor.py):
- μƒν” 1κ° μ²λ¦¬ μ‹κ°„: 35μ΄+
- GPU μ‚¬μ©λ¥ : 100%
- λ³‘λ©: μ΄λ―Έμ§€ μΈμ½”λ”© λ‹¨κ³„

μμƒ μ „μ²΄ ν•™μµ μ‹κ°„:
- 30κ° μƒν”: 35μ΄ Γ— 30 = 17.5λ¶„
- 1,000κ° μƒν”: 35μ΄ Γ— 1,000 = 9.7μ‹κ°„
- 138,000κ° μƒν”: 35μ΄ Γ— 138,000 = 1,340μ‹κ°„ (56μΌ!)
```

---

#### 3. Vision-Language λ¨λΈμ λ³Έμ§μ  λ³µμ΅μ„±

**μΌλ° LLM vs Vision-Language λ¨λΈ λΉ„κµ:**

| ν•­λ© | μΌλ° LLM (μ: Llama) | Vision-Language (Qwen2-VL) |
|------|---------------------|----------------------------|
| μ…λ ¥ | ν…μ¤νΈλ§ | μ΄λ―Έμ§€ + ν…μ¤νΈ |
| μ²λ¦¬ λ‹¨κ³„ | 1λ‹¨κ³„ (Language) | 3λ‹¨κ³„ (Vision β†’ Fusion β†’ Language) |
| λ©”λ¨λ¦¬ μ‚¬μ© | λ‚®μ | **λ†’μ** (μ΄λ―Έμ§€ activation) |
| ν•™μµ μ†λ„ | λΉ λ¦„ | **λλ¦Ό** (μ΄λ―Έμ§€ μΈμ½”λ”©) |
| LoRA ν¨κ³Ό | λ§¤μ° ν¨κ³Όμ  | μ ν•μ  (visionμ€ κ³ μ •) |

**Qwen2-VL μ•„ν‚¤ν…μ²:**
```
μ…λ ¥ μ΄λ―Έμ§€ (μ: 1024x1024)
    β†“
Vision Encoder (ViT-like)
    - Patch Embedding: μ΄λ―Έμ§€λ¥Ό ν¨μΉλ΅ λ¶„ν• 
    - Self-Attention: ν¨μΉ κ°„ κ΄€κ³„ ν•™μµ
    - μ¶λ ¥: κ°€λ³€ κΈΈμ΄ vision tokens (λ©”λ¨λ¦¬ λ§μ΄ μ‚¬μ©!)
    β†“
Vision-Language Fusion
    - Vision tokens + Text tokens κ²°ν•©
    - Cross-attention (λ©”λ¨λ¦¬ λ” λ§μ΄ μ‚¬μ©!)
    β†“
Language Model (Qwen2)
    - Autoregressive μƒμ„±
    - LoRAλ” μ—¬κΈ°λ§ μ μ© κ°€λ¥
```

**λ©”λ¨λ¦¬ κ³„μ‚° μμ‹:**
```python
# 1024x1024 μ΄λ―Έμ§€ ν•λ‚
image_size = 1024 * 1024 * 3  # RGB
patch_size = 14 * 14
num_patches = (1024 / 14) ** 2 = 5329 patches

# Vision encoder activation
hidden_dim = 1280
activation_size = num_patches * hidden_dim * 4 bytes (fp32)
                = 5329 * 1280 * 4 = 27MB per layer
                = 27MB * 32 layers = 864MB per image

# Gradient (backward pass)
gradient_size = activation_size * 2 = 1.7GB per image

# μ΄ λ©”λ¨λ¦¬: λ¨λΈ(1.5GB) + activation(0.9GB) + gradient(1.7GB) + ...
# = μ•½ 20GB+ for single image training!
```

---

#### 4. μ‹λ„ν• μµμ ν™”λ“¤κ³Ό μ‹¤ν¨ μ΄μ 

**A. Gradient Checkpointing λΉ„ν™μ„±ν™”**
```python
use_gradient_checkpointing=False
```
- **λ©μ **: μ†λ„ ν–¥μƒ
- **κ²°κ³Ό**: λ©”λ¨λ¦¬ λ” μ¦κ°€ (activation μ €μ¥)
- **κ²°λ΅ **: μ‹¤ν¨

**B. LoRA rank μ¶•μ† (r=16 β†’ r=8)**
```python
lora_config = LoraConfig(r=8, ...)
```
- **λ©μ **: ν•™μµ νλΌλ―Έν„° κ°μ†
- **κ²°κ³Ό**: νλΌλ―Έν„°λ” μ¤„μ—μ§€λ§ (4.3M β†’ 1.09M)
- **λ¬Έμ **: Vision encoder λ©”λ¨λ¦¬λ” κ·Έλ€λ΅
- **κ²°λ΅ **: ν¨κ³Ό λ―Έλ―Έ

**C. Vision layers κ³ μ •**
```python
finetune_vision_layers=False  # visionμ€ ν•™μµ μ• ν•¨
```
- **λ©μ **: Vision gradient μ κ±°
- **κ²°κ³Ό**: μ—¬μ „ν forward passμ—μ„ λ©”λ¨λ¦¬ μ‚¬μ©
- **λ¬Έμ **: Inferenceμ—λ„ vision encoder ν•„μ”
- **κ²°λ΅ **: μ‹¤ν¨

**D. λ°°μΉ ν¬κΈ° 1**
```python
per_device_train_batch_size=1
```
- **λ©μ **: λ©”λ¨λ¦¬ μµμ†ν™”
- **κ²°κ³Ό**: μ—¬μ „ν 24GB μ΄κ³Ό
- **λ¬Έμ **: μ΄λ―Έμ§€ ν•λ‚λ§μΌλ΅λ„ 20GB+ μ‚¬μ©
- **κ²°λ΅ **: ν•κ³„ λ„λ‹¬

---

#### 5. μ™ LoRAκ°€ μ—¬κΈ°μ„λ” ν¨κ³Όκ°€ μ—†λ”κ°€?

**LoRAμ μ¥μ :**
- β… ν•™μµ νλΌλ―Έν„° κ°μ†: 2.2B β†’ 1.09M (99.95% κ°μ†)
- β… λ¨λΈ λ©”λ¨λ¦¬ κ°μ†: ν¨κ³Ό μμ
- β… ν•™μµ μ†λ„ ν–¥μƒ: Language modelμ—μ„λ” ν¨κ³Όμ 

**ν•μ§€λ§ Vision-Languageμ—μ„λ”:**
- β Vision encoder activation: LoRAλ΅ μ¤„μΌ μ μ—†μ
- β μ΄λ―Έμ§€ μ²λ¦¬ μ‹κ°„: λ³€ν•μ§€ μ•μ
- β Fusion layer memory: μ—¬μ „ν ν•„μ”
- β οΈ **λ³‘λ©μ΄ Languageκ°€ μ•„λ‹λΌ Visionμ— μμ!**

**κ²°λ΅ :**
LoRAλ” Language λ¶€λ¶„λ§ ν¨μ¨ν™”. 
**Vision-Language λ¨λΈμ λ©”λ¨λ¦¬ λ³‘λ©μ€ Vision Encoderμ— μμ–΄μ„ LoRAλ΅ ν•΄κ²° λ¶κ°€.**

---

#### 6. 24GB GPUμ ν•κ³„

**RTX 4090 (24GB VRAM) μ‹¤μΈ΅:**
- Qwen2-VL 4bit: 1.5GB
- LoRA: μ¶”κ°€ 0.7GB
- **μ΄λ―Έμ§€ 1κ° ν•™μµ**: 20GB+
- **μ΄ν•©**: 22-24GB (ν•κ³„)

**ν•„μ” GPU:**
- μ•μ •μ  ν•™μµ: **40GB+ (A100, H100)**
- Gradient accumulationμΌλ΅ λ°°μΉ ν¬κΈ° λλ¦¬κΈ°: λ¶κ°€λ¥ (λ©”λ¨λ¦¬ μ΄λ―Έ ν’€)
- Multi-GPU: κ°€λ¥ν•μ§€λ§ λ³µμ΅λ„ μ¦κ°€

---

#### 7. κ²°λ΅ 

**λ΅μ»¬ νμΈνλ‹μ΄ μ• λλ” ν•µμ‹¬ μ΄μ :**
1. β **λ©”λ¨λ¦¬ λ¶€μ΅±**: 24GBλ΅λ” Qwen2-VL ν•™μµ λ¶κ°€
2. β **μ†λ„ λ¬Έμ **: μƒν”λ‹Ή 35μ΄ (μ „μ²΄ ν•™μµ 56μΌ)
3. β **Vision-Language λ³µμ΅μ„±**: LoRAλ΅ ν•΄κ²° μ• λ¨
4. β **λ„κµ¬ νΈν™μ„±**: Unsloth μµμ ν™”κ°€ Qwen2-VLκ³Ό μ¶©λ

**μ™ Colab/ν΄λΌμ°λ“λ¥Ό μ¶”μ²ν•λ”κ°€?**
- β… λ” ν° GPU (A100 40GB+)
- β… μµμ ν™”λ ν™κ²½ (Unslothκ°€ Colabμ—μ„ μ μ‘λ™)
- β… λΉ λ¥Έ λ„¤νΈμ›ν¬ (λ¨λΈ λ‹¤μ΄λ΅λ“)
- β… κ²€μ¦λ λ…ΈνΈλ¶ (λ‹¤λ¥Έ μ‚¬λλ“¤μ΄ μ„±κ³µν• μ½”λ“)

## π’΅ κ¶μ¥ μ†”λ£¨μ…

### λ°©λ²• 1: Google Colab μ‚¬μ© (κ°€μ¥ μ¶”μ²)
**μ¥μ :**
- Unsloth κ³µμ‹ λ…ΈνΈλ¶ μ‚¬μ© κ°€λ¥
- λ¬΄λ£ GPU (T4) μ κ³µ
- ν™κ²½ μ„¤μ • λ¶ν•„μ”
- νμΈνλ‹ μ†λ„ λΉ λ¦„

**μ§„ν–‰ λ°©λ²•:**
1. Colabμ—μ„ Unslothμ Qwen2-VL λ…ΈνΈλ¶ μ—΄κΈ°
2. λ°μ΄ν„° μ—…λ΅λ“ (Google Drive μ—°λ™)
3. λ…ΈνΈλ¶ μ‹¤ν–‰ β†’ μλ™ νμΈνλ‹
4. νμΈνλ‹λ λ¨λΈ λ‹¤μ΄λ΅λ“

**Colab λ…ΈνΈλ¶:**
- https://colab.research.google.com/drive/1vqHUq9R...
- Unsloth κ³µμ‹ Qwen2-VL λ…ΈνΈλ¶ κ²€μƒ‰

### λ°©λ²• 2: κΈ°μ΅΄ ν•κµ­μ–΄ OCR λ¨λΈ μ‚¬μ©
**λ€μ• λ¨λΈλ“¤:**
- **naver-clova-ix/donut-base-finetuned-cord-v2**
- **PleIAs/OCRonos-Qwen2-VL**
- **Upstageμ Document AI λ¨λΈλ“¤**

μ΄λ―Έ ν•κµ­μ–΄λ΅ ν•™μµλ λ¨λΈμ„ μ‚¬μ©ν•λ©΄ μ¶”κ°€ νμΈνλ‹ μ—†μ΄λ„ μΆ‹μ€ μ„±λ¥

### λ°©λ²• 3: ν΄λΌμ°λ“ GPU μ‚¬μ©
**μµμ…:**
- **Vast.ai**: μ‹κ°„λ‹Ή $0.20~$0.50
- **RunPod**: μ‹κ°„λ‹Ή $0.30~$0.70
- **Lambda Labs**: μ‹κ°„λ‹Ή $0.50~$1.10

30λ¶„~1μ‹κ°„μ΄λ©΄ νμΈνλ‹ μ™„λ£ κ°€λ¥

## π“ μμƒ νμΈνλ‹ ν¨κ³Ό

**ν„μ¬ λ² μ΄μ¤λΌμΈ (νμΈνλ‹ μ „):**
- μ™„μ „ μΌμΉ: 10%
- μ •κ·ν™” μΌμΉ: 35%
- κ³ μ μ‚¬λ„ 80%+: 40%

**νμΈνλ‹ ν›„ μμƒ (Colab/ν΄λΌμ°λ“μ—μ„):**
- **μµμ† λ©ν‘**: 50-60% (ν„μ¬μ 1.5λ°°)
- **ν„μ‹¤μ  λ©ν‘**: 70-80% (ν„μ¬μ 2λ°°)
- **μµμ  λ©ν‘**: 85-90% (ν„μ¬μ 2.3λ°°)

**λ΅μ»¬ νμΈνλ‹ μ‹¤ν¨λ΅ κ²€μ¦ λ¶κ°€**

## π€ λ‹¤μ λ‹¨κ³„

### μµμ… A: ColabμΌλ΅ μ§„ν–‰
```bash
# λ°μ΄ν„° μ••μ¶•
cd /root/data/deepseek_ocr
tar -czf korean_signboard_data.tar.gz train_qwen2vl.jsonl val_qwen2vl.jsonl filtered_train/images filtered_val/images
```
β†’ Google Drive μ—…λ΅λ“ β†’ Colab λ…ΈνΈλ¶ μ‹¤ν–‰

### μµμ… B: λ‹¤λ¥Έ λ¨λΈ ν…μ¤νΈ
κΈ°μ΅΄ ν•κµ­μ–΄ OCR λ¨λΈλ΅ λΉ„κµ ν‰κ°€

### μµμ… C: κ³„μ† λ΅μ»¬ μ‹λ„
λ” κ°„λ‹¨ν• λ¨λΈμ΄λ‚ λ‹¤λ¥Έ μ ‘κ·Ό λ°©μ‹ νƒμƒ‰

## π“ νμΌ μ •λ¦¬

**λ°μ΄ν„°:**
- `/root/data/deepseek_ocr/train_qwen2vl.jsonl` - ν•™μµ λ°μ΄ν„°
- `/root/data/deepseek_ocr/val_qwen2vl.jsonl` - κ²€μ¦ λ°μ΄ν„°
- `/root/data/deepseek_ocr/filtered_train/` - ν•™μµ μ΄λ―Έμ§€
- `/root/data/deepseek_ocr/filtered_val/` - κ²€μ¦ μ΄λ―Έμ§€

**ν‰κ°€:**
- `/root/data/qwen2vl_baseline_results.json` - μ΄κΈ° ν‰κ°€ (5% μ™„μ „ μΌμΉ)
- `/root/data/qwen2vl_improved_eval.json` - κ°μ„ λ ν‰κ°€ (35% μ •κ·ν™”, 40% μ μ‚¬λ„)

**μ‹λ„ν• ν•™μµ μ¤ν¬λ¦½νΈλ“¤ (λ¨λ‘ μ‹¤ν¨):**
- `/root/data/train_qwen2vl.py` - Unsloth λ°©μ‹ (Triton μ¤λ¥)
- `/root/data/train_qwen2vl_transformers.py` - Transformers (λ°μ΄ν„° μ²λ¦¬ μ‹¤ν¨)
- `/root/data/train_mini.py` - μ΄μ†ν• (OOM μ—λ¬)
- `/root/data/train_with_memory_monitor.py` - λ©”λ¨λ¦¬ λ¨λ‹ν„°λ§ (24GB ν•κ³„ ν™•μΈ)

**μ¤ν¬λ¦½νΈ:**
- `/root/data/evaluate_baseline.py` - ν‰κ°€ μ¤ν¬λ¦½νΈ
- `/root/data/test_qwen2vl_inference.py` - μ¶”λ΅  ν…μ¤νΈ
- `/root/data/convert_to_qwen2vl.py` - λ°μ΄ν„° λ³€ν™

---

## κ²°λ΅ 

λ΅μ»¬ ν™κ²½μ—μ„ Qwen2-VL νμΈνλ‹μ€ κΈ°μ μ μΌλ΅ κ°€λ¥ν•μ§€λ§ λ§¤μ° λ³µμ΅ν•©λ‹λ‹¤.
**Colab μ‚¬μ©μ„ κ°•λ ¥ν κ¶μ¥**ν•©λ‹λ‹¤ - λ¬΄λ£μ΄κ³ , λΉ λ¥΄κ³ , κ²€μ¦λ λ°©λ²•μ…λ‹λ‹¤.
