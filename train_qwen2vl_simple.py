#!/usr/bin/env python3
"""
Qwen2-VL íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸ (ê°„ì†Œí™” ë²„ì „)
ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ì²˜ë¦¬í•˜ê³  ì €ì¥í•˜ëŠ” ë°©ì‹
"""

import torch
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from transformers import BitsAndBytesConfig
from datasets import load_dataset
from PIL import Image
import json
from tqdm import tqdm
import os

print("ğŸ“¦ ì„¤ì • ì‹œì‘...")

# 4ë¹„íŠ¸ ì–‘ìí™” ì„¤ì •
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# ëª¨ë¸ ë¡œë“œ
print("ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...")
model = Qwen2VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2-VL-2B-Instruct",
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)

processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-2B-Instruct")

# ëª¨ë¸ ì¤€ë¹„
model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=False)

# LoRA ì„¤ì • (ë” ë³´ìˆ˜ì ì¸ ì„¤ì •)
print("ğŸ”§ LoRA ì ìš© ì¤‘...")
lora_config = LoraConfig(
    r=8,  # rank ë‚®ì¶¤
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],  # í•µì‹¬ ëª¨ë“ˆë§Œ íƒ€ê²Ÿ
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# LoRA íŒŒë¼ë¯¸í„°ê°€ í•™ìŠµ ê°€ëŠ¥í•œì§€ í™•ì¸
for name, param in model.named_parameters():
    if 'lora' in name.lower():
        param.requires_grad = True
        print(f"âœ“ {name}: requires_grad={param.requires_grad}")

# ë°ì´í„°ì…‹ ë¡œë“œ
print("\nğŸ“Š ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...")
with open("/root/data/deepseek_ocr/train_qwen2vl.jsonl", 'r', encoding='utf-8') as f:
    train_data = [json.loads(line) for line in f]

with open("/root/data/deepseek_ocr/val_qwen2vl.jsonl", 'r', encoding='utf-8') as f:
    val_data = [json.loads(line) for line in f]

# ìƒ˜í”Œë§Œ ì‚¬ìš© (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
train_samples = train_data[:50]  # 50ê°œë§Œ
val_samples = val_data[:10]  # 10ê°œë§Œ

print(f"- í•™ìŠµ ìƒ˜í”Œ: {len(train_samples)}ê°œ")
print(f"- ê²€ì¦ ìƒ˜í”Œ: {len(val_samples)}ê°œ")

# í•™ìŠµ í•¨ìˆ˜
def train_step(batch_data, model, optimizer):
    """ë‹¨ì¼ í•™ìŠµ ìŠ¤í…"""
    model.train()
    total_loss = 0
    
    for item in batch_data:
        messages = item["messages"]
        
        # ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        image_path = messages[0]["content"][0]["image"]
        user_text = messages[0]["content"][1]["text"]
        assistant_text = messages[1]["content"][0]["text"]
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        try:
            image = Image.open(image_path).convert("RGB")
        except Exception as e:
            print(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {image_path}")
            continue
        
        # ëŒ€í™” êµ¬ì„±
        conversation = [
            {
                "role": "user",
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": user_text}
                ]
            },
            {
                "role": "assistant",
                "content": [
                    {"type": "text", "text": assistant_text}
                ]
            }
        ]
        
        # í”„ë¡œì„¸ì‹±
        text_prompt = processor.apply_chat_template(conversation, tokenize=False)
        inputs = processor(
            text=[text_prompt],
            images=[image],
            padding=True,
            return_tensors="pt"
        ).to(model.device)
        
        # Forward pass
        outputs = model(**inputs, labels=inputs["input_ids"])
        loss = outputs.loss
        
        # Backward pass
        loss.backward()
        total_loss += loss.item()
    
    # Optimizer step
    optimizer.step()
    optimizer.zero_grad()
    
    return total_loss / len(batch_data)

# í‰ê°€ í•¨ìˆ˜
def evaluate(val_data, model):
    """ëª¨ë¸ í‰ê°€"""
    model.eval()
    total_loss = 0
    count = 0
    
    with torch.no_grad():
        for item in val_data:
            messages = item["messages"]
            
            # ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            image_path = messages[0]["content"][0]["image"]
            user_text = messages[0]["content"][1]["text"]
            assistant_text = messages[1]["content"][0]["text"]
            
            try:
                image = Image.open(image_path).convert("RGB")
            except:
                continue
            
            # ëŒ€í™” êµ¬ì„±
            conversation = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image"},
                        {"type": "text", "text": user_text}
                    ]
                },
                {
                    "role": "assistant",
                    "content": [
                        {"type": "text", "text": assistant_text}
                    ]
                }
            ]
            
            # í”„ë¡œì„¸ì‹±
            text_prompt = processor.apply_chat_template(conversation, tokenize=False)
            inputs = processor(
                text=[text_prompt],
                images=[image],
                padding=True,
                return_tensors="pt"
            ).to(model.device)
            
            # Forward pass
            outputs = model(**inputs, labels=inputs["input_ids"])
            total_loss += outputs.loss.item()
            count += 1
    
    return total_loss / max(count, 1)

# ì˜µí‹°ë§ˆì´ì € ì„¤ì •
print("\nğŸ”§ ì˜µí‹°ë§ˆì´ì € ì„¤ì •...")
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)

# í•™ìŠµ ì‹œì‘
print("\nğŸš€ í•™ìŠµ ì‹œì‘!")
num_epochs = 3
batch_size = 4

for epoch in range(num_epochs):
    print(f"\n{'='*50}")
    print(f"Epoch {epoch+1}/{num_epochs}")
    print(f"{'='*50}")
    
    # í•™ìŠµ
    epoch_loss = 0
    num_batches = (len(train_samples) + batch_size - 1) // batch_size
    
    for i in tqdm(range(0, len(train_samples), batch_size), desc="Training"):
        batch = train_samples[i:i+batch_size]
        loss = train_step(batch, model, optimizer)
        epoch_loss += loss
    
    avg_train_loss = epoch_loss / num_batches
    
    # í‰ê°€
    print("\nğŸ“Š ê²€ì¦ ì¤‘...")
    val_loss = evaluate(val_samples, model)
    
    print(f"âœ… Epoch {epoch+1} ì™„ë£Œ")
    print(f"   - í•™ìŠµ Loss: {avg_train_loss:.4f}")
    print(f"   - ê²€ì¦ Loss: {val_loss:.4f}")
    
    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    if (epoch + 1) % 1 == 0:
        save_path = f"/root/data/qwen2vl_finetuned/checkpoint-epoch-{epoch+1}"
        os.makedirs(save_path, exist_ok=True)
        model.save_pretrained(save_path)
        processor.save_pretrained(save_path)
        print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {save_path}")

# ìµœì¢… ëª¨ë¸ ì €ì¥
print("\nğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥ ì¤‘...")
final_path = "/root/data/qwen2vl_finetuned/final"
os.makedirs(final_path, exist_ok=True)
model.save_pretrained(final_path)
processor.save_pretrained(final_path)

print(f"\nâœ… í•™ìŠµ ì™„ë£Œ!")
print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {final_path}")
