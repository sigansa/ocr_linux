#!/usr/bin/env python3
"""
Qwen2-VL íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸ (ê¸°ë³¸ Transformers ì‚¬ìš©)
í•œêµ­ì–´ ê°„íŒ OCR í”„ë¡œì íŠ¸
"""

import torch
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from peft import LoraConfig, get_peft_model
from datasets import load_dataset
from transformers import Trainer, TrainingArguments
from PIL import Image
import json

print("ğŸ“¦ íŒ¨í‚¤ì§€ ë¡œë”© ì™„ë£Œ")

# ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ë¡œë“œ
print("ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...")
model = Qwen2VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2-VL-2B-Instruct",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True,
)

processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-2B-Instruct")

print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

# LoRA ì„¤ì •
print("ğŸ”§ LoRA ì„¤ì • ì¤‘...")
lora_config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# ë°ì´í„°ì…‹ ë¡œë“œ
print("ğŸ“Š ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...")
train_dataset = load_dataset("json", data_files="/root/data/deepseek_ocr/train_qwen2vl.jsonl", split="train")
val_dataset = load_dataset("json", data_files="/root/data/deepseek_ocr/val_qwen2vl.jsonl", split="train")

print(f"- í•™ìŠµ ìƒ˜í”Œ: {len(train_dataset)}ê°œ")
print(f"- ê²€ì¦ ìƒ˜í”Œ: {len(val_dataset)}ê°œ")

# ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_function(examples):
    """ë°ì´í„°ë¥¼ ëª¨ë¸ ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    batch = {"input_ids": [], "attention_mask": [], "labels": [], "pixel_values": [], "image_grid_thw": []}
    
    for messages in examples["messages"]:
        # ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        user_content = messages[0]["content"]
        assistant_content = messages[1]["content"]
        
        # ì´ë¯¸ì§€ ê²½ë¡œì™€ í…ìŠ¤íŠ¸ ë¶„ë¦¬
        image_path = user_content[0]["image"]
        user_text = user_content[1]["text"]
        assistant_text = assistant_content[0]["text"]
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        try:
            image = Image.open(image_path).convert("RGB")
        except:
            print(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {image_path}")
            continue
        
        # ëŒ€í™” í˜•ì‹ìœ¼ë¡œ êµ¬ì„±
        conversation = [
            {
                "role": "user",
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": user_text}
                ]
            },
            {
                "role": "assistant",
                "content": [
                    {"type": "text", "text": assistant_text}
                ]
            }
        ]
        
        # í”„ë¡œì„¸ì„œë¡œ ì¸ì½”ë”©
        text = processor.apply_chat_template(conversation, tokenize=False)
        inputs = processor(
            text=[text],
            images=[image],
            padding="max_length",
            max_length=512,
            truncation=True,
            return_tensors="pt"
        )
        
        batch["input_ids"].append(inputs["input_ids"][0])
        batch["attention_mask"].append(inputs["attention_mask"][0])
        batch["labels"].append(inputs["input_ids"][0])
        
        if "pixel_values" in inputs:
            batch["pixel_values"].append(inputs["pixel_values"][0])
        if "image_grid_thw" in inputs:
            batch["image_grid_thw"].append(inputs["image_grid_thw"][0])
    
    # í…ì„œë¡œ ë³€í™˜
    if len(batch["input_ids"]) > 0:
        batch["input_ids"] = torch.stack(batch["input_ids"])
        batch["attention_mask"] = torch.stack(batch["attention_mask"])
        batch["labels"] = torch.stack(batch["labels"])
        
        if len(batch["pixel_values"]) > 0:
            batch["pixel_values"] = torch.stack(batch["pixel_values"])
        if len(batch["image_grid_thw"]) > 0:
            batch["image_grid_thw"] = torch.stack(batch["image_grid_thw"])
    
    return batch

print("ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
# ìƒ˜í”Œë§Œ ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆì•½)
train_dataset_small = train_dataset.select(range(min(100, len(train_dataset))))
val_dataset_small = val_dataset.select(range(min(20, len(val_dataset))))

# í•™ìŠµ ì„¤ì •
training_args = TrainingArguments(
    output_dir="/root/data/qwen2vl_korean_signboard_v2",
    num_train_epochs=1,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=8,
    learning_rate=2e-4,
    logging_steps=5,
    eval_steps=10,
    save_steps=10,
    save_total_limit=2,
    bf16=True,
    report_to="none",
    remove_unused_columns=False,
)

print("ğŸš€ í•™ìŠµ ì‹œì‘!")
print(f"- í•™ìŠµ ìƒ˜í”Œ: {len(train_dataset_small)}ê°œ")
print(f"- ê²€ì¦ ìƒ˜í”Œ: {len(val_dataset_small)}ê°œ")
print(f"- ë°°ì¹˜ í¬ê¸°: {training_args.per_device_train_batch_size}")
print(f"- Gradient accumulation: {training_args.gradient_accumulation_steps}")
print(f"- ì´ effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")

# ê°„ë‹¨í•œ ë°ì´í„° ì½œë ˆì´í„°
def collate_fn(examples):
    """ë°°ì¹˜ ë°ì´í„° ê²°í•©"""
    return preprocess_function({"messages": [ex["messages"] for ex in examples]})

# íŠ¸ë ˆì´ë„ˆ ì„¤ì •
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset_small,
    eval_dataset=val_dataset_small,
    data_collator=collate_fn,
)

# í•™ìŠµ ì‹œì‘
trainer_stats = trainer.train()

# ëª¨ë¸ ì €ì¥
print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
model.save_pretrained("/root/data/qwen2vl_korean_signboard_v2/final")
processor.save_pretrained("/root/data/qwen2vl_korean_signboard_v2/final")

print("\nâœ… í•™ìŠµ ì™„ë£Œ!")
print(f"- í•™ìŠµ Loss: {trainer_stats.training_loss:.4f}")
print(f"- ì €ì¥ ìœ„ì¹˜: /root/data/qwen2vl_korean_signboard_v2/final")
