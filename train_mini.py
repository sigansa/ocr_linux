#!/usr/bin/env python3
"""
Qwen2-VL ì´ˆê°„ë‹¨ íŒŒì¸íŠœë‹
ìˆ˜ë™ í•™ìŠµ ë£¨í”„ë¡œ ìµœì†Œí•œì˜ íŒŒì¸íŠœë‹ë§Œ ìˆ˜í–‰
"""

import torch
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from transformers import BitsAndBytesConfig
from PIL import Image
import json
from tqdm import tqdm
import os

print("ğŸ“¦ ì„¤ì • ì‹œì‘...")

# 4ë¹„íŠ¸ ì–‘ìí™”
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# ëª¨ë¸ ë¡œë“œ
print("ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...")
model = Qwen2VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2-VL-2B-Instruct",
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)

processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-2B-Instruct")

# LoRA ì ìš©
print("ğŸ”§ LoRA ì ìš© ì¤‘...")
model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=False)

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, lora_config)
print(f"âœ… LoRA ì ìš© ì™„ë£Œ")
model.print_trainable_parameters()

# í•™ìŠµ ë°ì´í„° ë¡œë“œ (ì•„ì£¼ ì‘ì€ subset)
print("\nğŸ“Š ë°ì´í„° ë¡œë”© ì¤‘...")
with open("/root/data/deepseek_ocr/train_qwen2vl.jsonl", 'r', encoding='utf-8') as f:
    train_data = [json.loads(line) for line in f]

# 30ê°œë§Œ ì‚¬ìš©
train_samples = train_data[:30]
print(f"í•™ìŠµ ìƒ˜í”Œ: {len(train_samples)}ê°œ\n")

# ì˜µí‹°ë§ˆì´ì €
optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=1e-4)

# í•™ìŠµ
print("ğŸš€ í•™ìŠµ ì‹œì‘!")
num_epochs = 2
model.train()

for epoch in range(num_epochs):
    print(f"\n{'='*60}")
    print(f"Epoch {epoch+1}/{num_epochs}")
    print(f"{'='*60}")
    
    epoch_loss = 0
    processed = 0
    
    for idx, item in enumerate(tqdm(train_samples, desc=f"Epoch {epoch+1}")):
        try:
            messages = item["messages"]
            
            # ë°ì´í„° ì¶”ì¶œ
            image_path = messages[0]["content"][0]["image"]
            user_text = messages[0]["content"][1]["text"]
            assistant_text = messages[1]["content"][0]["text"]
            
            # ì´ë¯¸ì§€ ë¡œë“œ
            image = Image.open(image_path).convert("RGB")
            
            # ëŒ€í™” êµ¬ì„±
            conversation = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image"},
                        {"type": "text", "text": user_text}
                    ]
                },
                {
                    "role": "assistant",
                    "content": [
                        {"type": "text", "text": assistant_text}
                    ]
                }
            ]
            
            # í”„ë¡œì„¸ì‹±
            text_prompt = processor.apply_chat_template(conversation, tokenize=False)
            inputs = processor(
                text=[text_prompt],
                images=[image],
                padding=True,
                return_tensors="pt"
            ).to(model.device)
            
            # labels ì„¤ì •
            inputs["labels"] = inputs["input_ids"].clone()
            
            # Forward
            outputs = model(**inputs)
            loss = outputs.loss
            
            # Backward
            loss.backward()
            
            # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  (4 ìŠ¤í…ë§ˆë‹¤ ì—…ë°ì´íŠ¸)
            if (idx + 1) % 4 == 0:
                optimizer.step()
                optimizer.zero_grad()
            
            epoch_loss += loss.item()
            processed += 1
            
        except Exception as e:
            print(f"\nâš ï¸ ìƒ˜í”Œ {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            continue
    
    # ë‚¨ì€ ê·¸ë˜ë””ì–¸íŠ¸ ì ìš©
    optimizer.step()
    optimizer.zero_grad()
    
    avg_loss = epoch_loss / max(processed, 1)
    print(f"\nâœ… Epoch {epoch+1} ì™„ë£Œ - í‰ê·  Loss: {avg_loss:.4f}")

# ëª¨ë¸ ì €ì¥
print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
save_dir = "/root/data/qwen2vl_finetuned_mini"
os.makedirs(save_dir, exist_ok=True)

model.save_pretrained(save_dir)
processor.save_pretrained(save_dir)

print(f"âœ… ì €ì¥ ì™„ë£Œ: {save_dir}")
print("\nğŸ‰ íŒŒì¸íŠœë‹ ì™„ë£Œ!")
