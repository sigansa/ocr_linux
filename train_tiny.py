#!/usr/bin/env python3
"""
Qwen2-VL ì´ˆì†Œí˜• íŒŒì¸íŠœë‹ (10ê°œ ìƒ˜í”Œë§Œ)
ì‹¤ì œë¡œ ì™„ë£Œì‹œì¼œì„œ ì „í›„ ë¹„êµ
"""

import torch
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from transformers import BitsAndBytesConfig
from PIL import Image
import json
from tqdm import tqdm
import os

print("="*60)
print("ğŸ¯ ì´ˆì†Œí˜• íŒŒì¸íŠœë‹ (10ê°œ ìƒ˜í”Œ, ì‹¤ì œ ì™„ë£Œ ëª©í‘œ)")
print("="*60)

# 4ë¹„íŠ¸ ì–‘ìí™”
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

print("\nğŸ“¦ ëª¨ë¸ ë¡œë”© ì¤‘...")
model = Qwen2VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2-VL-2B-Instruct",
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)

processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-2B-Instruct")

# LoRA ì ìš©
print("ğŸ”§ LoRA ì ìš© ì¤‘...")
model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=False)

lora_config = LoraConfig(
    r=8,  # ì‘ê²Œ
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],  # í•µì‹¬ë§Œ
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# ë°ì´í„° ë¡œë“œ
print("\nğŸ“Š ë°ì´í„° ë¡œë”© ì¤‘...")
with open("/root/data/deepseek_ocr/train_qwen2vl.jsonl", 'r', encoding='utf-8') as f:
    train_data = [json.loads(line) for line in f]

# ë”± 10ê°œë§Œ
train_samples = train_data[:10]
print(f"í•™ìŠµ ìƒ˜í”Œ: {len(train_samples)}ê°œ")
print(f"ì˜ˆìƒ ì‹œê°„: ~2-3ë¶„\n")

# ì˜µí‹°ë§ˆì´ì €
optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=2e-4)

# í•™ìŠµ (1 epochë§Œ)
print("ğŸš€ í•™ìŠµ ì‹œì‘!")
print("-"*60)

model.train()
total_loss = 0
success_count = 0

for idx, item in enumerate(tqdm(train_samples, desc="í•™ìŠµ ì§„í–‰")):
    try:
        messages = item["messages"]
        
        # ë°ì´í„° ì¶”ì¶œ
        image_path = messages[0]["content"][0]["image"]
        user_text = messages[0]["content"][1]["text"]
        assistant_text = messages[1]["content"][0]["text"]
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        image = Image.open(image_path).convert("RGB")
        
        # ëŒ€í™” êµ¬ì„±
        conversation = [
            {
                "role": "user",
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": user_text}
                ]
            },
            {
                "role": "assistant",
                "content": [
                    {"type": "text", "text": assistant_text}
                ]
            }
        ]
        
        # í”„ë¡œì„¸ì‹±
        text_prompt = processor.apply_chat_template(conversation, tokenize=False)
        inputs = processor(
            text=[text_prompt],
            images=[image],
            padding=True,
            return_tensors="pt"
        ).to(model.device)
        
        inputs["labels"] = inputs["input_ids"].clone()
        
        # Forward & Backward
        outputs = model(**inputs)
        loss = outputs.loss
        loss.backward()
        
        # Update (ë§¤ ìƒ˜í”Œë§ˆë‹¤)
        optimizer.step()
        optimizer.zero_grad()
        
        total_loss += loss.item()
        success_count += 1
        
    except Exception as e:
        print(f"\nâš ï¸ ìƒ˜í”Œ {idx} ì‹¤íŒ¨: {e}")
        continue

avg_loss = total_loss / max(success_count, 1)

print(f"\nâœ… í•™ìŠµ ì™„ë£Œ!")
print(f"   - ì²˜ë¦¬ ìƒ˜í”Œ: {success_count}/{len(train_samples)}")
print(f"   - í‰ê·  Loss: {avg_loss:.4f}")

# ëª¨ë¸ ì €ì¥
print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
save_dir = "/root/data/qwen2vl_tiny_finetuned"
os.makedirs(save_dir, exist_ok=True)

model.save_pretrained(save_dir)
processor.save_pretrained(save_dir)

print(f"âœ… ì €ì¥ ì™„ë£Œ: {save_dir}")
print("\n" + "="*60)
print("ğŸ‰ íŒŒì¸íŠœë‹ ì™„ë£Œ! ì´ì œ ë¹„êµ í‰ê°€ë¥¼ ì§„í–‰í•˜ì„¸ìš”.")
print("="*60)
